#!/bin/bash
# Training for the transformer.

set -eou pipefail

readonly DATA=../../abstractness/data
readonly ARCH=transformer
# TODO: Set according to language.
readonly LANGUAGE=mri
readonly MODEL_DIR="../models/${ARCH}"

readonly TRAIN="${DATA}/${LANGUAGE}/${LANGUAGE}_train.tsv"
cat "${DATA}/${LANGUAGE}/${LANGUAGE}_"[0-7]".tsv" > "${TRAIN}"
trap "rm -f ${TRAIN}" EXIT
readonly VAL="${DATA}/${LANGUAGE}/${LANGUAGE}_8.tsv"

# Model parameters from Wu et al. ("A Smaller Transformer"):
# https://aclanthology.org/2021.eacl-main.163/
yoyodyne-train \
    --experiment "${LANGUAGE}" \
    --train "${TRAIN}" \
    --val "${VAL}" \
    --model_dir "${MODEL_DIR}" \
    --arch "${ARCH}" \
    --embedding_size 256 \
    --hidden_size 1024 \
    --batch_size 400 \
    --beta2 .98 \
    --decoder_layers 4 \
    --encoder_layers 4 \
    --check_val_every_n_epoch 16 \
    --log_every_n_step 2 \
    --label_smoothing .1 \
    --max_epochs 800 \
    --scheduler warmupinvsqrt \
    --warmup_steps 4000 \
    --dropout .3 \
    --seed 49 \
    --gradient_clip_val 3
    #--precision 16 \
    #--accelerator gpu
    #--log_wandb
